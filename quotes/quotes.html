<HTML>
<HEAD>
    <TITLE>Simon Šuster's favorite NLP quotes</TITLE>
    <link rel="stylesheet" href="../simon.css" type="text/css">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="keywords"
          content="simon,suster,PhD,NLP,Computational Linguistics,Computational Semantics,Parsing,Corpora,Machine Learning">
    <meta name="google-site-verification" content="oScrPVgv8qU2EAPakM79qoZoCcqvKg3B5phbJRfVMls"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
</HEAD>

<div class="container">
    <body>
    <h2>Favorite NLP quotes</h2>
    <ul>
        <li style="margin-bottom:30px;">
            <span class="title"><q>There is no use disputing with the translating machine. It will prevail.</q></span><br/>P. P. Troyanskii, 1947, based on his conception of automatic translation from 1933.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>Most statistical MT derives from IBM-style models (Brown et al., 1993), which ignore syntax and allow arbitrary word-to-word translation. Hence they are able to align any sentence pair, however mismatched. However, they have a tendency to translate long sentences into word salad. </q></span><br/> Jason Eisner, 2003.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>To learn, an entity must have several choices of behavior; a means of judging the success of its choice,
and a way of improving its judgment. It is difficult to design a computer of this sort and harder yet to program a present-day computer to behave this way, but it is possible in principle. To produce high-quality translations, a computer must be able to learn to manipulate language and meaning. When the relations between language and meaning are specified, no matter in how complicated a way; when the criteria of high-quality translation are outlined, with suggestions about how to improve the criteria; and when the mode of improvement for each criterion is formulated, a computer can be built to produce high-quality translations. With technique, critique, and improvement rules specified heuristically, machine translation is at hand. A child of four can do it—why not a machine?</q></span><br/>John F. Tinker, Learning and Translating by Machines, 1963.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>In principle, one could model the distribution of dependency parses
in any number of sensible or perverse ways.</q></span><br/>Jason Eisner, 1996.
        </li>


        <li style="margin-bottom:30px;">
            <span class="title"><q>Of course, linguists do not generally keep NLP in mind as they do their work, so not all of linguistics
should be expected to be useful.</q></span><br/>Noah A. Smith, Linguistic Structure Prediction, 2011.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>[...] we argue that meaningful, practical reductions in word error rate are hopeless. We point out that trigrams remain the de facto standard not because we don’t know how to beat them, but because no improvements justify the cost.</q></span><br/>Joshua Goodman, 2001.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>Neither the imagination of linguists nor the nature of text
data can be confined to the classical formalisms of machine learning.</q></span><br/>Noah A. Smith, Linguistic Structure Prediction, 2011.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>Increasing F-score is often not a scientific contribution but how you did it may be a scientific contribution.</q></span><br/>Mark Johnson, 2012.
        </li>

        <li style="margin-bottom:30px;">
            <span class="title"><q>[...] interpreting the world in the light of your preconceptions; those preconceptions then reinforce how you reinterpret your evidence, and those strengthen your preconception [...] The model is feeding itself, is eating its own waste.</q></span><br/>Jason Eisner, on the Baum-Welch algorithm.
        </li>

    </ul>

    </body>
</div>

</HTML>

